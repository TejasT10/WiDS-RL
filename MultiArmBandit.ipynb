{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arm:\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def pull(self):\n",
    "        return np.random.binomial(1, self.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiBandit:\n",
    "    def __init__(self, probs=[0.1, 0.2, 0.7, 0.5]):\n",
    "        self.__arms = [Arm(p) for p in probs]\n",
    "        self.__regret = 0\n",
    "        self.__maxp = max(probs)\n",
    "\n",
    "    def num_arms(self):\n",
    "        return len(self.__arms)\n",
    "\n",
    "    def pull(self, arm_num):\n",
    "        reward = self.__arms[arm_num].pull()\n",
    "        self.__regret += self.__maxp - self.__arms[arm_num].p\n",
    "        return reward\n",
    "\n",
    "    def regret(self):\n",
    "        return self.__regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyAlgorithm:\n",
    "    def __init__(self, num_arms, horizon, epsilon):\n",
    "        self.num_arms = num_arms\n",
    "        self.horizon = horizon\n",
    "        self.epsilon = epsilon\n",
    "        self.timestep = 0\n",
    "        self.arm_pulls = np.zeros(num_arms)\n",
    "        self.arm_rewards = np.zeros(num_arms)\n",
    "        self.regrets = np.zeros(horizon)\n",
    "\n",
    "    def give_best_arm(self):\n",
    "        return np.argmax(self.arm_rewards / (self.arm_pulls + 1e-6))\n",
    "\n",
    "    def select_arm(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.num_arms)\n",
    "        else:\n",
    "            return np.argmax(self.arm_rewards / (self.arm_pulls + 1e-6))\n",
    "\n",
    "    def run_algorithm(self, bandit):\n",
    "        for _ in range(self.horizon):\n",
    "            arm_to_pull = self.select_arm()\n",
    "            reward = bandit.pull(arm_to_pull)\n",
    "            self.arm_pulls[arm_to_pull] += 1\n",
    "            self.arm_rewards[arm_to_pull] += reward\n",
    "            self.timestep += 1\n",
    "            self.regrets[_] = bandit.regret()\n",
    "\n",
    "    def plot(self):\n",
    "        plt.plot(np.cumsum(self.regrets), label='Epsilon-Greedy')\n",
    "        plt.xlabel('Timesteps')\n",
    "        plt.ylabel('Total Regret')\n",
    "        plt.title('Total Regret vs Timesteps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBEpsilonAlgorithm:\n",
    "    def __init__(self, num_arms, horizon, c):\n",
    "        self.num_arms = num_arms\n",
    "        self.horizon = horizon\n",
    "        self.c = c\n",
    "        self.timestep = 0\n",
    "        self.arm_pulls = np.zeros(num_arms)\n",
    "        self.arm_rewards = np.zeros(num_arms)\n",
    "        self.regrets = np.zeros(horizon)\n",
    "\n",
    "    def give_best_arm(self):\n",
    "        return np.argmax(self.arm_rewards / (self.arm_pulls + 1e-6))\n",
    "\n",
    "    def select_arm(self):\n",
    "        if self.timestep < self.num_arms:\n",
    "            return self.timestep\n",
    "        else:\n",
    "            ucb_values = self.arm_rewards / (self.arm_pulls + 1e-6) + self.c * np.sqrt(np.log(self.timestep) / (self.arm_pulls + 1e-6))\n",
    "            return np.argmax(ucb_values)\n",
    "\n",
    "    def run_algorithm(self, bandit):\n",
    "        for _ in range(self.horizon):\n",
    "            arm_to_pull = self.select_arm()\n",
    "            reward = bandit.pull(arm_to_pull)\n",
    "            self.arm_pulls[arm_to_pull] += 1\n",
    "            self.arm_rewards[arm_to_pull] += reward\n",
    "            self.timestep += 1\n",
    "            self.regrets[_] = bandit.regret()\n",
    "\n",
    "    def plot(self):\n",
    "        plt.plot(np.cumsum(self.regrets), label='UCB Epsilon')\n",
    "        plt.xlabel('Timesteps')\n",
    "        plt.ylabel('Total Regret')\n",
    "        plt.title('Total Regret vs Timesteps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSamplingAlgorithm:\n",
    "    def __init__(self, num_arms, horizon):\n",
    "        self.num_arms = num_arms\n",
    "        self.horizon = horizon\n",
    "        self.timestep = 0\n",
    "        self.arm_pulls = np.zeros(num_arms)\n",
    "        self.arm_rewards = np.zeros(num_arms)\n",
    "        self.regrets = np.zeros(horizon)\n",
    "\n",
    "    def give_best_arm(self):\n",
    "        return np.argmax(self.arm_rewards / (self.arm_pulls + 1e-6))\n",
    "\n",
    "    def select_arm(self):\n",
    "        sampled_theta = np.random.beta(self.arm_rewards + 1, self.arm_pulls - self.arm_rewards + 1)\n",
    "        return np.argmax(sampled_theta)\n",
    "\n",
    "    def run_algorithm(self, bandit):\n",
    "        for _ in range(self.horizon):\n",
    "            arm_to_pull = self.select_arm()\n",
    "            reward = bandit.pull(arm_to_pull)\n",
    "            self.arm_pulls[arm_to_pull] += 1\n",
    "            self.arm_rewards[arm_to_pull] += reward\n",
    "            self.timestep += 1\n",
    "            self.regrets[_] = bandit.regret()\n",
    "\n",
    "    def plot(self):\n",
    "        plt.plot(np.cumsum(self.regrets), label='Thompson Sampling')\n",
    "        plt.xlabel('Timesteps')\n",
    "        plt.ylabel('Total Regret')\n",
    "        plt.title('Total Regret vs Timesteps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a MultiBandit instance\n",
    "bandit = MultiBandit()\n",
    "\n",
    "# Set the horizon size\n",
    "H = 100\n",
    "\n",
    "# Create instances of each algorithm\n",
    "epsilon_greedy_algorithm = EpsilonGreedyAlgorithm(num_arms=bandit.num_arms(), horizon=H, epsilon=0.2)\n",
    "ucb_epsilon_algorithm = UCBEpsilonAlgorithm(num_arms=bandit.num_arms(), horizon=H, c=2.0)\n",
    "thompson_sampling_algorithm = ThompsonSamplingAlgorithm(num_arms=bandit.num_arms(), horizon=H)\n",
    "\n",
    "# Run each algorithm\n",
    "epsilon_greedy_algorithm.run_algorithm(bandit)\n",
    "ucb_epsilon_algorithm.run_algorithm(bandit)\n",
    "thompson_sampling_algorithm.run_algorithm(bandit)\n",
    "\n",
    "# Display total regret for each algorithm\n",
    "print(f\"Epsilon-Greedy Algorithm - Total Regret after {H} timesteps: {bandit.regret()} with assumed best arm {epsilon_greedy_algorithm.give_best_arm()}\")\n",
    "print(f\"UCB Epsilon Algorithm - Total Regret after {H} timesteps: {bandit.regret()} with assumed best arm {ucb_epsilon_algorithm.give_best_arm()}\")\n",
    "print(f\"Thompson Sampling Algorithm - Total Regret after {H} timesteps: {bandit.regret()} with assumed best arm {thompson_sampling_algorithm.give_best_arm()}\")\n",
    "\n",
    "# Plot the regrets for each algorithm\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
